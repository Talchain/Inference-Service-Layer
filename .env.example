# Inference Service Layer Configuration
# Copy this file to .env and adjust values as needed

# API Configuration
API_V1_PREFIX=/api/v1
PROJECT_NAME="Olumi Inference Service Layer"
VERSION=0.1.0

# Server Configuration
HOST=0.0.0.0
PORT=8000
RELOAD=true
WORKERS=1

# Logging
LOG_LEVEL=INFO

# Computation Settings
DEFAULT_CONFIDENCE_LEVEL=0.95
MAX_MONTE_CARLO_ITERATIONS=10000
RESPONSE_TIMEOUT_SECONDS=30

# FACET Configuration
FACET_ENABLED=true
FACET_ROBUSTNESS_CHECKS=100

# Feature Flags
TEAM_ALIGNMENT_ENABLED=true
SENSITIVITY_ANALYSIS_ENABLED=true

# Determinism
ENABLE_DETERMINISTIC_MODE=true
