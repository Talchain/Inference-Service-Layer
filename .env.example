# Inference Service Layer Configuration
# Copy this file to .env and adjust values as needed

# ============================================================================
# SECURITY CONFIGURATION (REQUIRED FOR PRODUCTION)
# ============================================================================

# API Keys (comma-separated list)
# REQUIRED in production. Multiple keys allow key rotation without downtime.
# ISL_API_KEYS=key1,key2,key3

# CORS Origins (comma-separated list)
# REQUIRED in production. Must use HTTPS origins only.
# CORS_ORIGINS=https://olumi.app,https://app.olumi.app
CORS_ALLOW_CREDENTIALS=true

# Trusted Proxies (for X-Forwarded-For header validation)
# Required for accurate rate limiting behind load balancers
# TRUSTED_PROXIES=10.0.0.0/8,172.16.0.0/12,192.168.0.0/16

# ============================================================================
# API CONFIGURATION
# ============================================================================

API_V1_PREFIX=/api/v1
PROJECT_NAME="Olumi Inference Service Layer"
VERSION=0.1.0

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================

HOST=0.0.0.0
PORT=8000
RELOAD=true
WORKERS=1
ENVIRONMENT=development  # development, staging, production

# ============================================================================
# LOGGING
# ============================================================================

LOG_LEVEL=INFO

# Error Tracking (Sentry)
# SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id
# SENTRY_ENVIRONMENT=development  # development, staging, production
# SENTRY_TRACES_SAMPLE_RATE=0.1  # 10% of transactions
# SENTRY_PROFILES_SAMPLE_RATE=0.1  # 10% of transactions
# SENTRY_ENABLED=false  # Set to true to enable Sentry

# Computation Settings
DEFAULT_CONFIDENCE_LEVEL=0.95
MAX_MONTE_CARLO_ITERATIONS=10000
RESPONSE_TIMEOUT_SECONDS=30

# FACET Configuration
FACET_ENABLED=true
FACET_ROBUSTNESS_CHECKS=100

# Feature Flags
TEAM_ALIGNMENT_ENABLED=true
SENSITIVITY_ANALYSIS_ENABLED=true

# Determinism
ENABLE_DETERMINISTIC_MODE=true

# ============================================================================
# REDIS CONFIGURATION (REQUIRED FOR PRODUCTION)
# ============================================================================

# Redis URL (required for distributed rate limiting and caching)
# REDIS_URL=redis://localhost:6379/0

# Redis Authentication
# REDIS_PASSWORD=your-secure-redis-password

# Redis TLS (recommended for production)
# REDIS_TLS_ENABLED=true
# REDIS_TLS_CERT_REQS=required
# REDIS_TLS_CA_CERTS=/path/to/ca-cert.pem
# REDIS_TLS_CERTFILE=/path/to/client-cert.pem
# REDIS_TLS_KEYFILE=/path/to/client-key.pem

# Redis Connection Pool
# REDIS_MAX_CONNECTIONS=50
# REDIS_SOCKET_KEEPALIVE=true
# REDIS_HEALTH_CHECK_INTERVAL=30

# ============================================================================
# RATE LIMITING
# ============================================================================

RATE_LIMIT_REQUESTS_PER_MINUTE=100
# RATE_LIMIT_BURST_SIZE=20

# ============================================================================
# REQUEST LIMITS (DoS PROTECTION)
# ============================================================================

MAX_REQUEST_SIZE_MB=10
REQUEST_TIMEOUT_SECONDS=60

# ============================================================================
# LLM PROVIDERS (OPTIONAL - FOR DISCOVERY/EXPLANATIONS FEATURES)
# ============================================================================

# OpenAI Configuration
# LLM_OPENAI_API_KEY=sk-...
# LLM_OPENAI_ORG_ID=org-...
# LLM_OPENAI_MODEL=gpt-4o-mini
# LLM_OPENAI_MAX_TOKENS=2000
# LLM_OPENAI_TEMPERATURE=0.7

# Anthropic Configuration
# LLM_ANTHROPIC_API_KEY=sk-ant-...
# LLM_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# LLM_ANTHROPIC_MAX_TOKENS=4000
# LLM_ANTHROPIC_TEMPERATURE=0.7

# LLM Features
# LLM_ENABLED=false
# LLM_COST_TRACKING_ENABLED=true
# LLM_CACHE_ENABLED=true
# LLM_CACHE_TTL_SECONDS=3600

# ============================================================================
# MONITORING & OBSERVABILITY
# ============================================================================

# Prometheus Metrics
# PROMETHEUS_ENABLED=true
# PROMETHEUS_PORT=9090

# Distributed Tracing
# TRACING_ENABLED=true
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================

# Circuit Breaker
# CIRCUIT_BREAKER_ENABLED=true
# MEMORY_THRESHOLD_PERCENT=85

# Cache Settings
# CACHE_TTL_SECONDS=3600
# CACHE_MAX_SIZE=1000

# ============================================================================
# EXTERNAL SERVICE INTEGRATION (OPTIONAL)
# ============================================================================

# PLoT Integration
# PLOT_API_URL=https://plot-api.olumi.com
# PLOT_API_KEY=your-plot-api-key

# TAE Integration
# TAE_API_URL=https://tae-api.olumi.com
# TAE_API_KEY=your-tae-api-key

# CEE Integration
# CEE_API_URL=https://cee-api.olumi.com
# CEE_API_KEY=your-cee-api-key
